{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11111820,"sourceType":"datasetVersion","datasetId":6927819}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T05:56:22.330820Z","iopub.execute_input":"2025-03-21T05:56:22.331683Z","iopub.status.idle":"2025-03-21T05:56:34.371952Z","shell.execute_reply.started":"2025-03-21T05:56:22.331650Z","shell.execute_reply":"2025-03-21T05:56:34.371272Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_dir = '/kaggle/input/skin-cancer-img-augmented/skincancerimg/train'\ntest_dir = '/kaggle/input/skin-cancer-img-augmented/skincancerimg/test'\n\n# Image dimensions and batch size\nIMG_SIZE = (380, 380)  # EfficientNetB4 input size\nBATCH_SIZE = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T05:57:09.012733Z","iopub.execute_input":"2025-03-21T05:57:09.013035Z","iopub.status.idle":"2025-03-21T05:57:09.017091Z","shell.execute_reply.started":"2025-03-21T05:57:09.013014Z","shell.execute_reply":"2025-03-21T05:57:09.016165Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode=\"categorical\"\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode=\"categorical\",\n    shuffle=False\n)\n\n# Get number of classes\nNUM_CLASSES = len(train_generator.class_indices)\nNUM_CLASSES","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T05:57:10.931594Z","iopub.execute_input":"2025-03-21T05:57:10.931885Z","iopub.status.idle":"2025-03-21T05:57:17.320150Z","shell.execute_reply.started":"2025-03-21T05:57:10.931863Z","shell.execute_reply":"2025-03-21T05:57:17.319394Z"}},"outputs":[{"name":"stdout","text":"Found 4500 images belonging to 9 classes.\nFound 1080 images belonging to 9 classes.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"9"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Load EfficientNetB4 model\nbase_model = EfficientNetB4(weights=\"imagenet\", include_top=False, input_shape=(380, 380, 3))\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.3)(x)  # Helps prevent overfitting\noutput = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n\n# Create model\nmodel = Model(inputs=base_model.input, outputs=output)\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T05:57:21.595029Z","iopub.execute_input":"2025-03-21T05:57:21.595337Z","iopub.status.idle":"2025-03-21T05:57:27.087193Z","shell.execute_reply.started":"2025-03-21T05:57:21.595314Z","shell.execute_reply":"2025-03-21T05:57:27.086532Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n\u001b[1m71686520/71686520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n# Define callbacks\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\", restore_best_weights=True)\n\n# Train model\nEPOCHS = 25  # Adjust as needed\nhistory = model.fit(\n    train_generator,\n    validation_data=test_generator,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[early_stopping]\n)\n\n# Evaluate model\ntest_loss, test_acc = model.evaluate(test_generator)\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T05:57:29.651000Z","iopub.execute_input":"2025-03-21T05:57:29.651335Z","iopub.status.idle":"2025-03-21T06:46:17.171887Z","shell.execute_reply.started":"2025-03-21T05:57:29.651311Z","shell.execute_reply":"2025-03-21T06:46:17.171166Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 2s/step - accuracy: 0.3128 - loss: 1.8818 - val_accuracy: 0.3352 - val_loss: 1.9374\nEpoch 2/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - accuracy: 0.7195 - loss: 0.8308 - val_accuracy: 0.5417 - val_loss: 1.4394\nEpoch 3/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 1s/step - accuracy: 0.8697 - loss: 0.4327 - val_accuracy: 0.6250 - val_loss: 1.2066\nEpoch 4/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 1s/step - accuracy: 0.9278 - loss: 0.2307 - val_accuracy: 0.6481 - val_loss: 1.1291\nEpoch 5/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - accuracy: 0.9532 - loss: 0.1499 - val_accuracy: 0.6574 - val_loss: 1.1573\nEpoch 6/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 1s/step - accuracy: 0.9482 - loss: 0.1487 - val_accuracy: 0.6463 - val_loss: 1.3339\nEpoch 7/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.9692 - loss: 0.0965 - val_accuracy: 0.6648 - val_loss: 1.2470\nEpoch 8/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.9642 - loss: 0.1050 - val_accuracy: 0.6426 - val_loss: 1.3860\nEpoch 9/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1s/step - accuracy: 0.9697 - loss: 0.0783 - val_accuracy: 0.6833 - val_loss: 1.3672\nEpoch 10/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1s/step - accuracy: 0.9730 - loss: 0.0701 - val_accuracy: 0.6380 - val_loss: 1.4659\nEpoch 11/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 1s/step - accuracy: 0.9698 - loss: 0.0713 - val_accuracy: 0.6667 - val_loss: 1.3280\nEpoch 12/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - accuracy: 0.9692 - loss: 0.0619 - val_accuracy: 0.6722 - val_loss: 1.4347\nEpoch 13/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - accuracy: 0.9726 - loss: 0.0623 - val_accuracy: 0.6491 - val_loss: 1.5702\nEpoch 14/25\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 1s/step - accuracy: 0.9679 - loss: 0.0696 - val_accuracy: 0.6722 - val_loss: 1.4337\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 538ms/step - accuracy: 0.6136 - loss: 1.2936\nTest Accuracy: 0.6481\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}